{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get preinstalled collections of data to work upon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus - Large collection of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will download the datasets\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# brown corpus has different categories of data...\n",
    "print(brown.categories())\n",
    "len(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adventure is a class, these data are labelled\n",
    "- we can make a topic based classifier which will tell us whether the sentence belongs to 'adventure','fiction','government'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='fiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of lists\n",
    "- each list is a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4249"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "#we have 4249 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each sentence is broken into  a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scotty accepted the decision with indifference and did not enter the arguments .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the data by joining with space\n",
    "' '.join(data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the data/corpus\n",
    "- Tokenisation, Stopward removal\n",
    "- Stemming/Lemmatization\n",
    "- Build a vocab\n",
    "- Vectorisation\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation, Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"I hope you're pleased with yourselves. We could all have been killed — or worse, expelled.\n",
    "                Now if you don't mind, I'm going to bed.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at vineetchanana@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I hope you're pleased with yourselves.\", 'We could all have been killed — or worse, expelled.', \"Now if you don't mind, I'm going to bed.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sents)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vineetchanana@gmail.com']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using split to break into words \n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it didnot separate 1,2,3, if we want to split, pass the separater = ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'vineetchanana', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it also break about the '@' special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "- either we can use our own stopwords or import from nl toolit (it has some common stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want stopwords of language english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'only', 'ma', 'against', 'being', 'who', 'not', 'once', 'over', 'more', 'will', \"haven't\", 'than', \"you're\", 'same', 'down', 'from', 'been', \"should've\", 'out', 'your', 'but', 'doesn', 'mustn', 'my', 'don', \"shouldn't\", 'this', 'were', 'did', 'most', 'during', 'it', 'of', \"you've\", 'do', 'there', 'few', 's', 'how', \"mustn't\", 'isn', 'as', 'you', 'through', 'all', \"couldn't\", 'myself', 'the', 'yourselves', 'about', \"wouldn't\", \"shan't\", 'those', \"isn't\", 'until', 'under', 'itself', \"won't\", 'had', 'then', 'no', 'while', 'their', 'on', 'aren', 'they', \"wasn't\", \"you'll\", 'between', 'some', 'ourselves', 'be', 'having', 'very', 'now', \"hasn't\", 't', \"mightn't\", 'what', 'just', \"that'll\", 'me', 'doing', 'them', 'or', 'am', 'shan', \"didn't\", \"she's\", 'o', 'y', 'her', 'himself', 'into', 'll', 're', 'own', 'whom', 'mightn', 'ain', 'these', 'does', 'so', 'for', 'haven', 'at', 'theirs', 'after', 'won', 'she', 'wouldn', 'in', 'are', 'why', 'other', 'when', 'hadn', 'can', 'because', \"don't\", 'i', 'each', \"it's\", 'to', 'and', 'has', 'such', 'should', 'by', 'up', \"weren't\", 'yours', 'below', 'hers', 'again', 'here', 'we', 'off', 'further', 'above', 'weren', 'couldn', 'he', 'his', 'our', 'a', 'have', \"hadn't\", 'yourself', 'needn', 'which', 'an', 'shouldn', \"needn't\", 'him', 'both', 'wasn', 'any', 'too', \"you'd\", 'herself', 'is', 'with', 'before', 'nor', \"aren't\", 'didn', 'themselves', 'if', \"doesn't\", 'd', 've', 'that', 'was', 'its', 'm', 'hasn', 'where'}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i am not bothered about her very much.'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words = remove_stopwords(text,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bothered', 'much.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but it also removed the word 'not', it changed the context of sentence as it removed the negation\n",
    "- if you want 'not' put a condition that word should not be 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at vineetchanana@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vineetchanana@gmail.com']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we are getting numbers, we donot want them as they are not meaningful\n",
    "- we can't use stopwords or any harcoded logic to remove numbers as there are infinite numbers\n",
    "- therefore we will use regular expressions to extract words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[a-zA-Z]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_text = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'vineetchanana', 'gmail', 'com']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(useful_text)\n",
    "len(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we removed all the numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to preserve gmail id, add '@' and '.' to regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = '[a-zA-Z@.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'vineetchanana@gmail.com']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = RegexpTokenizer(pattern2)\n",
    "usefultext2 = tokenizer2.tokenize(sentence)\n",
    "print(usefultext2)\n",
    "len(usefultext2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "- Process that transforms particular words(verbs,plurals) into their radical form\n",
    "- Preserve the semantics of the sentence without increasing number of unique tokens (which will help in reducing number of unique features\n",
    "- Example: jumps,jumping,jumped,jump => jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer,Porter , Lancaster Stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an object of this class\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rules bne hue h like 'ing' hoga to remove kr denge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snowball is a multi-lingual stemmer(Can also stem french, german words)\n",
    "ss = SnowballStemmer('english')  # as it is a multi-lingual need to mention the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('loves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will also produce the very similar results\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('jumps')\n",
    "#need to install this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building  a Vocab and Vecorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I’m selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can’t handle me at my worst, then you sure as hell don’t deserve me at my best.\",\n",
    "    \"A successful man is one who can lay a firm foundation with the bricks others have thrown at him.\",\n",
    "    \"I can’t give you a sure-fire formula for success, but I can give you a formula for failure: try to please everybody all the time.\",\n",
    "    \"It is hard to fail, but it is worse never to have tried to succeed.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This corpus have 4 documents, we will make a feature vector for each document so that we can classify the documents\n",
    "- if a word will come twice we will put 2 at that index\n",
    "- feature vector will be able to tell us that what words came in the document and how many times they came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an object\n",
    "cv = CountVectorizer()\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "# fit will learn the dictionary(vocab) from the corpus and tranform will convert the data into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x62 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse matrix\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 1 3 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 2 1 0 1 0 1 1 1 0 0 0 1 1 0 2\n",
      "  1 2 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 2]\n",
      " [0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 2 0 0 0 1 0 1 1 0 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 2 2 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 3 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#cobvert sparse matrix to array\n",
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus))\n",
    "# we can see we have 4 feature vectors (docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if you want to find out how mapping is done\n",
    "- cv has a attribute 'vocabulary' that got learned during fit method, will tell us the mapping of the word and the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'selfish': 44, 'impatient': 27, 'and': 2, 'little': 32, 'insecure': 28, 'make': 33, 'mistakes': 36, 'am': 1, 'out': 42, 'of': 39, 'control': 9, 'at': 4, 'times': 53, 'hard': 22, 'to': 54, 'handle': 21, 'but': 7, 'if': 26, 'you': 61, 'can': 8, 'me': 35, 'my': 37, 'worst': 60, 'then': 50, 'sure': 48, 'as': 3, 'hell': 24, 'don': 11, 'deserve': 10, 'best': 5, 'successful': 47, 'man': 34, 'is': 29, 'one': 40, 'who': 57, 'lay': 31, 'firm': 16, 'foundation': 19, 'with': 58, 'the': 49, 'bricks': 6, 'others': 41, 'have': 23, 'thrown': 51, 'him': 25, 'give': 20, 'fire': 15, 'formula': 18, 'for': 17, 'success': 46, 'failure': 14, 'try': 56, 'please': 43, 'everybody': 12, 'all': 0, 'time': 52, 'it': 30, 'fail': 13, 'worse': 59, 'never': 38, 'tried': 55, 'succeed': 45}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see the length of the feature vector is equal to the length of the vocaabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse mapping\n",
    "- if you want to convert data from numbers to text\n",
    "- given a list of numbers, how to find a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['all', 'but', 'can', 'everybody', 'failure', 'fire', 'for',\n",
       "        'formula', 'give', 'please', 'success', 'sure', 'the', 'time',\n",
       "        'to', 'try', 'you'], dtype='<U10')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see the words are shuffled, that's why it is a bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-14cb175f8c86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    # this tokenizer is our custom regular expressions based tokenizer\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    # lower the words so that Indian and indian means the same\n",
    "    \n",
    "    #remove stopwords\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to chapters 1,2,3 at vineetchanana@gmail.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['send', 'documents', 'related', 'chapters', 'vineetchanana', 'gmail', 'com']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence)\n",
    "myTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can pass our own tokenizer\n",
    "cv2 = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus2 = cv2.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))\n",
    "len(vectorized_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using our own tokenizer we reduced the length of the vector...makes our data represntable,memory friendly, reduces the dimensions, makes our data meaningful. \n",
    "- it is preferable to have bcoz if we have 1 lakh words, our performance will degrade because we are not doing stemming, clubbing similar words together then it will be difficult to generalise over new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['best', 'control', 'deserve', 'handle', 'hard', 'hell',\n",
       "        'impatient', 'insecure', 'little', 'make', 'mistakes', 'selfish',\n",
       "        'sure', 'times', 'worst'], dtype='<U10'),\n",
       " array(['bricks', 'firm', 'foundation', 'lay', 'man', 'one', 'others',\n",
       "        'successful', 'thrown'], dtype='<U10'),\n",
       " array(['everybody', 'failure', 'fire', 'formula', 'give', 'please',\n",
       "        'success', 'sure', 'time', 'try'], dtype='<U10'),\n",
       " array(['fail', 'hard', 'never', 'succeed', 'tried', 'worse'], dtype='<U10')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.inverse_transform(vectorized_corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for test data\n",
    "- we will have the same dictionary of training data, we will not learn new vocab for test data we will only use vocab that was learned during training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\n",
    "    \"impatient and a little insecure.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this case would you expect our vocab to have only 4 words, in ML both training and test data should have similar representation. We can not shrink the vocabulary\n",
    "- that's why we will only transform the data and not fit(learn new parameters for data)\n",
    "- donot use fit_transform as it will override our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ways to create features\n",
    "- Unigram - using every word as a feature\n",
    "- Bigram - using 2 consecutive words as a feature\n",
    "- Trigram - using 3 consecutive words as a feature\n",
    "- n-gram - Combine all the unigram,bigram,trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = ['this is good movie']\n",
    "sent_2 = ['this is good movie but actor is not present']\n",
    "sent_3 = ['this is not good movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use unigram features for the classifier it might possible that classifier predicts that 3rd sentence is a positive review because in first two reviews 'good' and 'movie' both words are present and they are both +ve reviews, so it might predict 3rd as +ve too, it's difficult to catch negation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= [sent_1[0],sent_2[0],sent_3[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 2, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 7, 'is': 3, 'good': 2, 'movie': 4, 'but': 1, 'actor': 0, 'not': 5, 'present': 6}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 8, 'is good': 3, 'good movie': 2, 'movie but': 5, 'but actor': 1, 'actor is': 0, 'is not': 4, 'not present': 7, 'not good': 6}\n",
      "\n",
      "Length of vocabulary: 9\n"
     ]
    }
   ],
   "source": [
    "##bigram\n",
    "cv2 = CountVectorizer(ngram_range=(2,2))\n",
    "cv2.fit_transform(docs)\n",
    "print(cv2.vocabulary_)\n",
    "print('\\nLength of vocabulary:', len(cv2.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we are getting 'is good' and 'not good' both...we are capturing negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is good': 8, 'is good movie': 3, 'good movie but': 2, 'movie but actor': 6, 'but actor is': 1, 'actor is not': 0, 'is not present': 5, 'this is not': 9, 'is not good': 4, 'not good movie': 7}\n",
      "\n",
      "Length of vocabulary: 10\n"
     ]
    }
   ],
   "source": [
    "##trigram\n",
    "cv3 = CountVectorizer(ngram_range=(3,3))\n",
    "cv3.fit_transform(docs)\n",
    "print(cv3.vocabulary_)\n",
    "print('\\nLength of vocabulary:', len(cv3.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 23, 'is': 9, 'good': 6, 'movie': 15, 'this is': 24, 'is good': 10, 'good movie': 7, 'this is good': 25, 'is good movie': 11, 'but': 3, 'actor': 0, 'not': 18, 'present': 22, 'movie but': 16, 'but actor': 4, 'actor is': 1, 'is not': 12, 'not present': 21, 'good movie but': 8, 'movie but actor': 17, 'but actor is': 5, 'actor is not': 2, 'is not present': 14, 'not good': 19, 'this is not': 26, 'is not good': 13, 'not good movie': 20}\n",
      "\n",
      "Length of vocabulary: 27\n"
     ]
    }
   ],
   "source": [
    "##ngram -combining all the features\n",
    "cv4 = CountVectorizer(ngram_range=(1,3))\n",
    "cv4.fit_transform(docs)\n",
    "print(cv4.vocabulary_)\n",
    "print('\\nLength of vocabulary:', len(cv4.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Length of vocabulary is getting increased...we have to keep a check on the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Normalisation\n",
    "\n",
    "- avoids features that occur very often, because they contain less information\n",
    "- information decreases as the number of occurences inccreases across all the documents\n",
    "- So we define another term term-document-frequency which associates a weight with every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not good movie\"\n",
    "\n",
    "corpus = [sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this was good movie', 'this is not good movie']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an object\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.inverse_transform(vc[2])[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see 'not' has maximum weight in 2nd doc because other words are also occuring other documents\n",
    "- in 1st doc, 'was' has higher weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
